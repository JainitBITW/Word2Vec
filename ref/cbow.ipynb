{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import pickle as pkl\n",
    "from scipy.spatial.distance import cosine\n",
    "# import cuda \n",
    "import torch.cuda as cuda\n",
    "\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        input_embeds = self.embeddings(inputs)\n",
    "        embeds = torch.mean(input_embeds, dim=1)\n",
    "        out = self.linear(embeds)\n",
    "        return F.log_softmax(out, dim=1)\n",
    "\n",
    "\n",
    "class Word2Vec: \n",
    "    def __init__(self,data_file, word2idx, context_size=2,embedding_size=50, oov_threshold=2, neg_sample_size=5, lr=0.5):\n",
    "        self.data_file = data_file\n",
    "        self.word2idx = word2idx\n",
    "        self.freq = pkl.load(open('freq.pkl', 'rb'))\n",
    "        self.freq_dist = np.array(list(self.freq.values()))\n",
    "        self.vocabulary = list(self.word2idx.keys())\n",
    "        self.context_size = context_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.oov_threshold = oov_threshold\n",
    "        self.neg_sample_size = neg_sample_size\n",
    "        self.lr = lr\n",
    "        self.BATCH_SIZE = 64\n",
    "        self.oov_token = '<OOV>'\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "        self.model = CBOW(self.vocab_size, self.embedding_size)\n",
    "        self.weights = self.negative_sampling()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.loss_function = nn.NLLLoss()\n",
    "        self.dataset = self.create_dataset()\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    \n",
    "    def create_dataset(self):\n",
    "        print(\"Creating Dataset\")\n",
    "        dataset = []\n",
    "        with open(self.data_file, 'r') as f:\n",
    "            for line in f:\n",
    "                tokens = eval(line)\n",
    "\n",
    "                for i in range(self.context_size, len(tokens) - self.context_size):\n",
    "                    focus_index = tokens[i]\n",
    "                    context_indices = []\n",
    "                    for j in range(i - self.context_size, i + self.context_size + 1):\n",
    "                        if i == j:\n",
    "                            continue\n",
    "                        context_index = tokens[j]\n",
    "                        context_indices.append(context_index)\n",
    "                    dataset.append((context_indices, focus_index))\n",
    "        return dataset\n",
    "    \n",
    "    def negative_sampling(self):\n",
    "        normalized_freq = F.normalize(\n",
    "            torch.Tensor(self.freq_dist).pow(0.75), dim=0)\n",
    "        weights = torch.ones(len(self.freq_dist)).cuda()\n",
    "\n",
    "        for _ in range(len(self.freq_dist)):\n",
    "            for _ in range(self.neg_sample_size):\n",
    "                neg_index = torch.multinomial(normalized_freq, 1)[0]\n",
    "                weights[neg_index] += 1\n",
    "\n",
    "        return weights\n",
    "    \n",
    "    def train(self, num_epochs):\n",
    "        self.model.to(self.device)\n",
    "        print(self.device)\n",
    "        losses = []\n",
    "        loss_fn = nn.NLLLoss(weight=self.weights)\n",
    "        for epoch in range(num_epochs):\n",
    "            if epoch % 2 == 0 and epoch != 0 and self.optimizer.param_groups[0]['lr'] > 0.001:\n",
    "                self.optimizer.param_groups[0]['lr'] /= 2\n",
    "                print(f\"changed Learning Rate: {self.optimizer.param_groups[0]['lr']}\")\n",
    "            print(f\"Epoch {epoch}\")\n",
    "            net_loss = 0\n",
    "            for i in range(0, len(self.dataset), self.BATCH_SIZE):\n",
    "                batch = self.dataset[i: i + self.BATCH_SIZE]\n",
    "\n",
    "                context = [x[0] for x in batch]\n",
    "                focus = [x[1] for x in batch]\n",
    "\n",
    "                context_var = Variable(torch.cuda.LongTensor(context))\n",
    "                focus_var = Variable(torch.cuda.LongTensor(focus))\n",
    "                context_var = context_var.to(self.device)\n",
    "                focus_var = focus_var.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                log_probs = self.model(context_var)\n",
    "                loss = loss_fn(log_probs, focus_var)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                net_loss += loss.item()\n",
    "            print(f\"Loss: {loss.item()}\")\n",
    "            losses.append(net_loss)\n",
    "\n",
    "    def get_embedding(self , word_idx):\n",
    "        embedding_index = Variable(torch.cuda.LongTensor([word_idx]))\n",
    "        return self.model.embeddings(embedding_index).data[0]\n",
    "    \n",
    "    def get_similar_words(self, word, topn=10):\n",
    "        word_idx = self.word2idx[word]\n",
    "        word_embedding = self.get_embedding(word_idx)\n",
    "        similarities = []\n",
    "        for i in self.word2idx.values():\n",
    "            if i == word_idx:\n",
    "                continue\n",
    "            sim = cosine(word_embedding, self.get_embedding(i))\n",
    "            similarities.append((self.vocabulary[i], sim))\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:topn]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = pkl.load(open('word2idx.pkl', 'rb'))\n",
    "encoder = Word2Vec('processed_data.txt', word2idx, context_size=2, embedding_size=50, oov_threshold=2, neg_sample_size=5, lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding( encoder,word_idx):\n",
    "        embedding_index = Variable(torch.cuda.LongTensor([word_idx]))\n",
    "        return encoder.model.embeddings(embedding_index).data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec={}\n",
    "for w,v in encoder.word2idx.items():\n",
    "    word2vec[w] = get_embedding(encoder,v).tolist()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.pkl', 'wb') as file:\n",
    "    pkl.dump(word2vec,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
